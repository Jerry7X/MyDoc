一些值得改进的地方：

+ 使用rados存放bucket index
rados对象并发粒度大，恢复粒度大，强一致性对rgw可用性造成了非常大的影响。

1亿个对象的bucket，如果osd异常，可能仅写了几百个对象，但是可能有几千万key需要恢复。


+ 通过hash方式访问对象，不经过bucket index
缺乏中间层，实现小文件合并(天生要求index层),对象跨pool都比较麻烦。

rados对象做index层几乎不可取。并发粒度太大，性能上抗不住。
如果通过对象属性来实现，就需要引入虚拟对象，这样rados对象数据在单个pool内剧增，其一致性保证也比较麻烦(rados没有跨对象的事务机制)

多版本的复杂实现就是其中一个例子（参考: rgw一致性）

3，元数据分散，需自行实现各种查询
甚至很多简单的查询命令都得额外开发。
无法支持各种灵活的视图。

4，multisite同步基于hash的shard
这样hash reshard之后，同步的元数据都需要调整。

5，杜绝覆盖写情况

对象改写情况本来就偏少，可以考虑保留其元数据，作为审计日志。

好处：
避免写之前的读属性操作
读写流程简化
恢复优化(恢复时只要对象存在就不需要恢复，恢复不必阻塞io，可以在io路径上直接完成恢复)

6, 小文件合并

使用SSD cache可得到优化，但如果能小文件合并，减少inode，仍然有价值，特别是SSD容量不足的情况下。
对于元数据独立存放的系统更是如此，给ssd划分分区做不同目的使用会使得管理复杂，且故障影响扩散。


优化：
优化list，以及各种方式的查询
面向bucket无限扩展，异构多数据中心，可扩展pool(故障域)，集群，异构集群设计。

远程数据中心之间完全同构对于一些场景可能不需要，用户可能期望每个站点知道全局数据，但是数据可以不在本地，访问时抓取（或远程访问）
还有一种一中心多站点的结构，这种结构中心也需要知道所有数据，但是也不一定存储了所有数据。
访问都是就近访问的原则。

可扩展集群，支持bucket跨集群，rgw管理多个集群，当一个集群使用接近9X%时。新对象不再写入该集群。

异构集群指的是rgw底下可以使用rados，也可以使用其他存储，比如NAS，比如本地做RAID的磁盘，甚至是公有云(pool里面丢对象与bucket里面丢对象的区别)

可支持数据在不同zone，集群，pool之间流动，迁移。 启动task在rgw上，发起同步task，stop task。
同步与向其他对象存储系统写入没有区别，所以底下支持的存储模型的接口是共用的。主动推送数据到其他地方。
这种同步方式灵活，好控制，状态维护简单，实现也相对简单(共用读写流程)




